---
# CephFS for applications
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: ceph-fs-main
  namespace: rook-ceph
spec:
  preservePoolNames: true # Preserve pool names
  metadataPool:
    name: cephfs-meta-main-rep3
    enableRBDStats: true # Enable RBD stats for monitoring
    failureDomain: osd
    replicated:
      size: 3
      requireSafeReplicaSize: true # Requires 3 OSDs
      hybridStorage:
        primaryDeviceClass: nvme
        secondaryDeviceClass: hdd
    parameters:
      #pg_num: "16"
      pg_num_min: "16"
  dataPools:
    # For Applications (Rep 3)
    - name: app-data-hdd-rep3
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      replicated:
        size: 3
        requireSafeReplicaSize: true
      parameters:
        #pg_num: "16"
        pg_num_min: "16"
      deviceClass: hdd

    # For Databases - CloudNativePG, YugabyteDB (Rep 3 on HDD)
    - name: db-data-hdd-rep3
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      replicated:
        size: 3
        requireSafeReplicaSize: true # Default, ensures 3 OSDs exist
      parameters:
        #pg_num: "16"
        pg_num_min: "16" # Higher PG count likely needed for DBs eventually
        min_size: "2" # Minimum number of replicas required for writes

     # For Databases (EC 2+1 on HDD)
    - name: db-data-hdd-ec21 # For Databases
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      erasureCoded:
        dataChunks: 2
        codingChunks: 1 # Requires 3+ OSDs to become active
      parameters:
        #pg_num: "16"
        pg_num_min: "16" # Higher PG count likely needed for DBs eventually
        min_size: "2" # Minimum number of replicas required for writes
      deviceClass: hdd

    # For Media Data (EC 2+1 on HDD) - Used by CephFS
    - name: media-data-hdd-ec21
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      erasureCoded:
        dataChunks: 2
        codingChunks: 1 # Requires 3+ OSDs to become active
      parameters:
        #pg_num: "16"
        pg_num_min: "16"
      deviceClass: hdd

  metadataServer:
    activeCount: 1
    activeStandby: true
    resources: # Keep your MDS resources
      requests: { cpu: "35m", memory: "64M" }
    # placement: # Keep your MDS placement constraints
    #   podAntiAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       - labelSelector: { matchExpressions: [{ key: app, operator: In, values: [rook-ceph-mds] }] }
    #         topologyKey: kubernetes.io/hostname
