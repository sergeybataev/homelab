---
# CephFS for applications
apiVersion: ceph.rook.io/v1
kind: CephFilesystem
metadata:
  name: main-fs
  namespace: rook-ceph
spec:
  preservePoolNames: true # Preserve pool names
  metadataPool:
    name: main-fs-meta
    enableRBDStats: true # Enable RBD stats for monitoring
    failureDomain: osd
    replicated:
      size: 3
      requireSafeReplicaSize: true # Requires 3 OSDs
    parameters:
      pg_autoscale_mode: "on"
      min_size: "2" # Minimum number of replicas required for writes

  dataPools:
    # For Applications (Rep 3)
    - name: main-fs-rep3-data
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      replicated:
        size: 3
        requireSafeReplicaSize: true
      parameters:
        pg_autoscale_mode: "on"
        min_size: "2" # Minimum number of replicas required for writes

    # For Database Data (EC 3+1 on HDD) - Used by CephFS
    - name: main-fs-db-ec31-data
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      erasureCoded:
        dataChunks: 3
        codingChunks: 1 # Requires 4+ OSDs to become active
      parameters:
        pg_autoscale_mode: "on"
        fast_read: "true"

    # For Media Data (EC 3+1 on HDD) - Used by CephFS
    - name: main-fs-ec31-data
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      erasureCoded:
        dataChunks: 3
        codingChunks: 1 # Requires 4+ OSDs to become active
      parameters:
        pg_autoscale_mode: "on"
        fast_read: "true"

    - name: main-fs-ec51-data
      enableRBDStats: true # Enable RBD stats for monitoring
      failureDomain: osd
      erasureCoded:
        dataChunks: 5
        codingChunks: 1 # Requires 5+ OSDs to become active
      parameters:
        # pg_num: "32"
        # pg_num_min: "32" # Higher PG count likely needed for DBs eventually
        pg_autoscale_mode: "on"
        fast_read: "true"


  metadataServer:
    activeCount: 1
    activeStandby: true
    resources: # Keep your MDS resources
      requests: { cpu: "35m", memory: "64M" }
    # placement: # Keep your MDS placement constraints
    #   podAntiAffinity:
    #     requiredDuringSchedulingIgnoredDuringExecution:
    #       - labelSelector: { matchExpressions: [{ key: app, operator: In, values: [rook-ceph-mds] }] }
    #         topologyKey: kubernetes.io/hostname
