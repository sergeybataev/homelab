---
# yaml-language-server: $schema=https://kubernetes-schemas.pages.dev/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name rook-ceph-cluster
  namespace: &ns rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.14.4
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: flux-system

  maxHistory: 3

  install:
    timeout: 30m
    replace: true
    crds: CreateReplace
    createNamespace: true
    remediation:
      retries: 300

  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3

  test:
    enable: true

  # rollback:
  #   recreate: true
  #   force: true
  #   cleanupOnFail: true

  uninstall:
    keepHistory: false

  values:
    operatorNamespace: *ns

    clusterName: *name

    # Installs a debugging toolbox deployment
    toolbox:
      # -- Enable Ceph debugging pod deployment. See [toolbox](../Troubleshooting/ceph-toolbox.md)
      enabled: true

    monitoring:
      # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
      # Monitoring requires Prometheus to be pre-installed
      enabled: true
      # -- Whether to create the Prometheus rules for Ceph alerts
      createPrometheusRules: true

    # -- Create & use PSP resources. Set this to the same value as the rook-ceph chart.
    pspEnable: false

    ingress:
      # -- Enable an ingress for the ceph-dashboard
      dashboard:
        ingressClassName: external
        annotations:
          external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN}
        host:
          name: &host rook.${SECRET_DOMAIN}
          path: "/"


    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true
      osd_pool_default_size = 1

    # All values below are taken from the CephCluster CRD
    # -- Cluster configuration.
    # @default -- See [below](#ceph-cluster-spec)
    cephClusterSpec:
      # This cluster spec example is for a converged cluster where all the Ceph daemons are running locally,
      # as in the host-based example (cluster.yaml). For a different configuration such as a
      # PVC-based cluster (cluster-on-pvc.yaml), external cluster (cluster-external.yaml),
      # or stretch cluster (cluster-stretched.yaml), replace this entire `cephClusterSpec`
      # with the specs from those examples.

      # The path on the host where configuration files will be persisted. Must be specified.
      # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
      # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
      dataDirHostPath: /var/lib/rook

      mon:
        # Set the number of mons to be started. Generally recommended to be 3.
        # For highest availability, an odd number of mons should be specified.
        count: 3
        # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
        # Mons should only be allowed on the same node for test environments where data loss is acceptable.
        # allowMultiplePerNode: true

      mgr:
        # When higher availability of the mgr is needed, increase the count to 2.
        # In that case, one mgr will be active and one in standby. When Ceph updates which
        # mgr is active, Rook will update the mgr services to match the active mgr.
        count: 2
        # allowMultiplePerNode: true
        modules:
          # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
          # are already enabled by other settings in the cluster CR.
          - name: pg_autoscaler
            enabled: true
          - name: rook
            enabled: true

      # enable the crash collector for ceph daemon crash collection
      crashCollector:
        disable: false
        # Uncomment daysToRetain to prune ceph crash entries older than the
        # specified number of days.
        # daysToRetain: 30

      logCollector:
        enabled: false

      # enable the ceph dashboard for viewing cluster status
      dashboard:
        enabled: true

        # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
        urlPrefix: /
        # serve the dashboard at the given port.
        # port: 8443
        # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
        # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
        ssl: false

      # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
      cleanupPolicy:
        # Since cluster cleanup is destructive to data, confirmation is required.
        # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
        # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
        # Rook will immediately stop configuring the cluster and only wait for the delete command.
        # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
        confirmation: ""
        # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
        sanitizeDisks:
          # method indicates if the entire disk should be sanitized or simply ceph's metadata
          # in both case, re-install is possible
          # possible choices are 'complete' or 'quick' (default)
          method: quick
          # dataSource indicate where to get random bytes from to write on the disk
          # possible choices are 'zero' (default) or 'random'
          # using random sources will consume entropy from the system and will take much more time then the zero source
          dataSource: zero
          # iteration overwrite N times instead of the default (1)
          # takes an integer value
          iteration: 1
        # allowUninstallWithVolumes defines how the uninstall should be performed
        # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
        allowUninstallWithVolumes: false

      # To control where various services will be scheduled by kubernetes, use the placement configuration sections below.
      # The example under 'all' would have all services scheduled on kubernetes nodes labeled with 'role=storage-node' and
      # tolerate taints with a key of 'storage-node'.
      # placement:
      #   all:
      #     nodeAffinity:
      #       requiredDuringSchedulingIgnoredDuringExecution:
      #         nodeSelectorTerms:
      #           - matchExpressions:
      #             - key: role
      #               operator: In
      #               values:
      #               - storage-node
      #     podAffinity:
      #     podAntiAffinity:
      #     topologySpreadConstraints:
      #     tolerations:
      #     - key: storage-node
      #       operator: Exists
      placement:
        all: &placement
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    - key: storage.io/ebs
                      operator: In
                      values:
                        - "true"

      # # The option to automatically remove OSDs that are out and are safe to destroy.
      # removeOSDsIfOutAndSafeToRemove: false

      # # priority classes to apply to ceph resources
      # priorityClassNames:
      #   mon: system-node-critical
      #   osd: system-node-critical
      #   mgr: system-cluster-critical

      storage:
        # when onlyApplyOSDPlacement is false, will merge both placement.All() and storageClassDeviceSets.Placement.
        onlyApplyOSDPlacement: false

        useAllNodes: false
        useAllDevices: false

        config:
          osdsPerDevice: "1"
        nodes:
          - name: "main.storage"
            devices:
              - name: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S4EWNG0M218299R"
          - name: "rpi-5-1-green"
            devices:
              - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_EVO_2TB_S7GDNS0X203177N"
          - name: "rpi-5-2-yellow"
            devices:
              - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_EVO_1TB_S7GCNS0X121015K"
        # storageClassDeviceSets:
        #   - name: ceph-on-zfs
        #     # The number of OSDs to create from this device set
        #     count: 1

        #     # IMPORTANT: If volumes specified by the storageClassName are not portable across nodes
        #     # this needs to be set to false. For example, if using the local storage provisioner
        #     # this should be false.
        #     portable: false

        #     # Certain storage class in the Cloud are slow
        #     # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
        #     # Currently, "gp2" has been identified as such
        #     tuneDeviceClass: false # local ZFS is pretty fast...

        #     # Certain storage class in the Cloud are fast
        #     # Rook can configure the OSD running on PVC to accommodate that by tuning some of the Ceph internal
        #     # Currently, "managed-premium" has been identified as such
        #     tuneFastDeviceClass: true

        #     # whether to encrypt the deviceSet or not
        #     encrypted: false

        #     # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
        #     # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
        #     # as soon as you have more than one OSD per node. The topology spread constraints will
        #     # give us an even spread on K8s 1.18 or newer.
        #     placement:
        #       topologySpreadConstraints:
        #         - maxSkew: 1
        #           topologyKey: kubernetes.io/hostname
        #           whenUnsatisfiable: ScheduleAnyway
        #           labelSelector:
        #             matchExpressions:
        #               - key: app
        #                 operator: In
        #                 values:
        #                   - rook-ceph-osd

        #     # (still under storage class deviceset)
        #     preparePlacement:
        #       podAntiAffinity:
        #         preferredDuringSchedulingIgnoredDuringExecution:
        #           - weight: 100
        #             podAffinityTerm:
        #               labelSelector:
        #                 matchExpressions:
        #                   - key: app
        #                     operator: In
        #                     values:
        #                       - rook-ceph-osd
        #                   - key: app
        #                     operator: In
        #                     values:
        #                       - rook-ceph-osd-prepare
        #               topologyKey: kubernetes.io/hostname
        #       topologySpreadConstraints:
        #         - maxSkew: 1
        #           # IMPORTANT: If you don't have zone labels, change this to another key such as kubernetes.io/hostname
        #           #topologyKey: topology.kubernetes.io/zone
        #           topologyKey: kubernetes.io/hostname
        #           whenUnsatisfiable: DoNotSchedule
        #           labelSelector:
        #             matchExpressions:
        #               - key: app
        #                 operator: In
        #                 values:
        #                   - rook-ceph-osd-prepare

            # (still under stoarge class deviceset)
            # These are the OSD daemon limits. For OSD prepare limits, see the separate section below for "prepareosd" resources


      resources:
        mgr:
          requests:
            cpu: "125m"
            memory: "512Mi"
          limits:
            memory: "1Gi"
        mon:
          # This is one of the tweaks - normally you'd want more than one monitor and
          # you'd want to spread them out
          requests:
            cpu: "50m"
            memory: "512Mi"
          limits:
            memory: "1Gi"
        osd:
          requests:
            cpu: "300m"
            memory: "512Mi"
          limits:
            memory: "6Gi"
        mgr-sidecar:
          requests:
            cpu: "50m"
            memory: "100Mi"
          limits:
            memory: "200Mi"
        # crashcollector:
        #   requests:
        #     cpu: "15m"
        #     memory: "64Mi"
        #   limits:
        #     memory: "128Mi"
        logcollector:
          requests:
            cpu: "100m"
            memory: "100Mi"
          limits:
            memory: "1Gi"
        prepareosd:
          # limits: It is not recommended to set limits on the OSD prepare job
          #         since it's a one-time burst for memory that must be allowed to
          #         complete without an OOM kill.  Note however that if a k8s
          #         limitRange guardrail is defined external to Rook, the lack of
          #         a limit here may result in a sync failure, in which case a
          #         limit should be added.  1200Mi may suffice for up to 15Ti
          #         OSDs ; for larger devices 2Gi may be required.
          #         cf. https://github.com/rook/rook/pull/11103
          requests:
            cpu: "250m"
            memory: "50Mi"
          # limits:
          #   memory: "2Gi"
        cleanup:
          requests:
            cpu: "250m"
            memory: "100Mi"
          limits:
            memory: "1Gi"

    # disable defaults
    cephBlockPools: []

    # # -- A list of CephBlockPool configurations to deploy
    # # @default -- See [below](#ceph-block-pools)
    # cephBlockPools:
    #   - name: ceph-blockpool-replicated
    #     spec:
    #       failureDomain: host
    #       replicated:
    #         size: 1
    #       # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
    #       # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
    #       enableRBDStats: true

    #     storageClass:
    #       enabled: true
    #       name: ceph-block-test
    #       isDefault: true
    #       reclaimPolicy: Delete
    #       allowVolumeExpansion: true

    #       # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
    #       allowedTopologies: []
    #       #        - matchLabelExpressions:
    #       #            - key: rook-ceph-role
    #       #              values:
    #       #                - storage-node

    #       # see https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/Block-Storage-RBD/block-storage.md#provision-storage for available configuration
    #       parameters:
    #         # RBD image format. Defaults to "2".
    #         imageFormat: "2"

    #         # RBD image features, equivalent to OR'd bitfield value: 63
    #         # Available for imageFormat: "2". Older releases of CSI RBD
    #         # support only the `layering` feature. The Linux kernel (KRBD) supports the
    #         # full feature complement as of 5.4.0.
    #         imageFeatures: layering

    #         csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
    #         csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
    #         csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
    #         csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
    #         csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
    #         csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"

    #         # Specify the filesystem type of the volume. If not specified, csi-provisioner
    #         # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    #         # in hyperconverged settings where the volume is mounted on the same node as the osds.
    #         csi.storage.k8s.io/fstype: ext4

    cephFileSystems:
      - name: ceph-filesystem
        spec:
          metadataPool:
            replicated:
              size: 3
          dataPools:
            - failureDomain: host
              erasureCoded:
                dataChunks: 2
                codingChunks: 1
              name: data0
          metadataServer:
            activeCount: 1
            activeStandby: true
            # priorityClassName: system-cluster-critical
            resources:
              requests:
                cpu: "35m"
                memory: "64M"
              limits:
                memory: "144M"
        storageClass:
          enabled: true
          isDefault: false
          name: ceph-filesystem
          pool: data0
          reclaimPolicy: Delete
          allowVolumeExpansion: true
          # mountOptions: []
          volumeBindingMode: Immediate
          parameters:
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: storage
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: storage
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: storage
            csi.storage.k8s.io/fstype: ext4

    # # -- A list of CephFileSystem configurations to deploy
    # # @default -- See [below](#ceph-file-systems)
    # cephFileSystems:
    #   - name: ceph-filesystem-test
    #     # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
    #     spec:
    #       metadataPool:
    #         replicated:
    #           size: 1
    #           requireSafeReplicaSize: false # remove after increase replica size
    #       dataPools:
    #         - failureDomain: host
    #           replicated:
    #             size: 1
    #             requireSafeReplicaSize: false # remove after increase replica size
    #           name: data0
    #           compressionMode: none
    #       metadataServer:
    #         activeCount: 1
    #         activeStandby: false # change to true after increase replica size
    #         priorityClassName: system-cluster-critical
    #         resources:
    #           requests:
    #             cpu: 1000m
    #             memory: 4Gi
    #           limits:
    #             memory: 4Gi

    #     storageClass:
    #       enabled: true
    #       isDefault: false
    #       name: ceph-filesystem-test
    #       pool: data0
    #       reclaimPolicy: Delete
    #       allowVolumeExpansion: true
    #       parameters:
    #         csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
    #         csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
    #         csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
    #         csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
    #         csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
    #         csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
    #         # Specify the filesystem type of the volume. If not specified, csi-provisioner
    #         # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
    #         # in hyperconverged settings where the volume is mounted on the same node as the osds.
    #         csi.storage.k8s.io/fstype: ext4

    # # -- Settings for the filesystem snapshot class
    # # @default -- See [CephFS Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#cephfs-snapshots)
    # cephFileSystemVolumeSnapshotClass:
    #   enabled: false
    #   name: ceph-filesystem-test
    #   isDefault: true
    #   deletionPolicy: Delete
    #   annotations: {}
    #   labels: {}
    #   # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#cephfs-snapshots for available configuration
    #   parameters: {}

    # # -- Settings for the block pool snapshot class
    # # @default -- See [RBD Snapshots](../Storage-Configuration/Ceph-CSI/ceph-csi-snapshot.md#rbd-snapshots)
    # cephBlockPoolsVolumeSnapshotClass:
    #   enabled: false
    #   name: ceph-block-test
    #   isDefault: false
    #   deletionPolicy: Delete
    #   annotations: {}
    #   labels: {}
    #   # see https://rook.io/docs/rook/v1.10/Storage-Configuration/Ceph-CSI/ceph-csi-snapshot/#rbd-snapshots for available configuration
    #   parameters: {}

    cephObjectStores:
      - name: ceph-objectstore
        spec:
          metadataPool:
            failureDomain: host
            replicated:
              size: 2
          dataPool:
            failureDomain: host
            # For production it is recommended to use more chunks, such as 4+2 or 8+4
            erasureCoded:
              dataChunks: 2
              codingChunks: 1
          preservePoolsOnDelete: false
          gateway:
            #securePort: 443
            port: 80
            # resources:
            #   requests:
            #     cpu: 1000m
            #     memory: 1Gi
            #   limits:
            #     memory: 2Gi
            instances: 1
            priorityClassName: system-cluster-critical
          healthCheck:
            bucket:
              interval: 60s
        storageClass:
          enabled: true
          name: ceph-bucket
          reclaimPolicy: Delete
          volumeBindingMode: Immediate
          parameters:
            region: eu-central-1
        ingress:
          enabled: true
          ingressClassName: external
          annotations:
            external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN}
            nginx.ingress.kubernetes.io/proxy-body-size: "0"
            nginx.ingress.kubernetes.io/proxy-request-buffering: "off"
          host:
            name: rgw.${SECRET_DOMAIN}
            path: /

    # cephECBlockPools are disabled by default, please remove the comments and set desired values to enable it
    # For erasure coded a replicated metadata pool is required.
    # https://rook.io/docs/rook/latest/CRDs/Shared-Filesystem/ceph-filesystem-crd/#erasure-coded
    cephECBlockPools:
    - name: ceph-block-ec-pool
      spec:
        metadataPool:
          replicated:
            size: 2
        dataPool:
          failureDomain: host
          erasureCoded:
            dataChunks: 2
            codingChunks: 1

      parameters:
        # clusterID is the namespace where the rook cluster is running
        # If you change this namespace, also change the namespace below where the secret namespaces are defined
        clusterID: rook-ceph # namespace:cluster
        # (optional) mapOptions is a comma-separated list of map options.
        # For krbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
        # For nbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
        # mapOptions: lock_on_read,queue_depth=1024

        # (optional) unmapOptions is a comma-separated list of unmap options.
        # For krbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd/#kernel-rbd-krbd-options
        # For nbd options refer
        # https://docs.ceph.com/docs/latest/man/8/rbd-nbd/#options
        # unmapOptions: force

        # RBD image format. Defaults to "2".
        imageFormat: "2"

        # RBD image features, equivalent to OR'd bitfield value: 63
        # Available for imageFormat: "2". Older releases of CSI RBD
        # support only the `layering` feature. The Linux kernel (KRBD) supports the
        # full feature complement as of 5.4
        # imageFeatures: layering,fast-diff,object-map,deep-flatten,exclusive-lock
        imageFeatures: layering

      storageClass:
        provisioner: rook-ceph.rbd.csi.ceph.com # csi-provisioner-name
        enabled: true
        name: ceph-block
        isDefault: true
        annotations: { }
        labels: { }
        allowVolumeExpansion: true
        reclaimPolicy: Delete
        volumeBindingMode: Immediate

# -- CSI driver name prefix for cephfs, rbd and nfs.
# @default -- `namespace name where rook-ceph operator is deployed`
