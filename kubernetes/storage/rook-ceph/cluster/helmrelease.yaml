---
# yaml-language-server: $schema=https://raw.githubusercontent.com/fluxcd-community/flux2-schemas/main/all.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name rook-ceph-cluster
  namespace: &ns rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      chart: rook-ceph-cluster
      version: v1.16.5
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
        namespace: flux-system

  maxHistory: 3

  install:
    timeout: 30m
    replace: true
    crds: CreateReplace
    createNamespace: true
    remediation:
      retries: 300

  upgrade:
    cleanupOnFail: true
    crds: CreateReplace
    remediation:
      retries: 3

  test:
    enable: true

  # rollback:
  #   recreate: true
  #   force: true
  #   cleanupOnFail: true

  uninstall:
    keepHistory: false

  values:
    operatorNamespace: *ns
    clusterName: rook-ceph
    toolbox:
      enabled: true

    # --- MONITORING ---
    monitoring:
      # -- Enable Prometheus integration, will also create necessary RBAC rules to allow Operator to create ServiceMonitors.
      # Monitoring requires Prometheus to be pre-installed
      enabled: true
      # -- Whether to create the Prometheus rules for Ceph alerts
      createPrometheusRules: true

    # -- Create & use PSP resources. Set this to the same value as the rook-ceph chart.
    pspEnable: false

    ingress:
      # -- Enable an ingress for the ceph-dashboard
      dashboard:
        ingressClassName: external
        annotations:
          external-dns.alpha.kubernetes.io/target: external.${SECRET_DOMAIN}
        host:
          name: &host rook.${SECRET_DOMAIN}
          path: "/"


    configOverride: |
      [global]
      bdev_enable_discard = true
      bdev_async_discard = true
      # Remove osd_pool_default_size = 1 - Let pool definitions control size

    cephClusterSpec:
      # This cluster spec example is for a converged cluster where all the Ceph daemons are running locally,
      # as in the host-based example (cluster.yaml). For a different configuration such as a
      # PVC-based cluster (cluster-on-pvc.yaml), external cluster (cluster-external.yaml),
      # or stretch cluster (cluster-stretched.yaml), replace this entire `cephClusterSpec`
      # with the specs from those examples.

      # The path on the host where configuration files will be persisted. Must be specified.
      # Important: if you reinstall the cluster, make sure you delete this directory from each host or else the mons will fail to start on the new cluster.
      # In Minikube, the '/data' directory is configured to persist across reboots. Use "/data/rook" in Minikube environment.
      dataDirHostPath: /var/lib/rook

      # network:
      #   # The network provider to use for the Ceph daemons. The default is "host" which uses the host network.
      #   # also support "multus" for multus CNI plugin.
      #   provider: "host"

      mon:
        # Set the number of mons to be started. Generally recommended to be 3.
        # For highest availability, an odd number of mons should be specified.
        count: 3

        # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
        # Mons should only be allowed on the same node for test environments where data loss is acceptable.
        allowMultiplePerNode: false

      mgr:
        # When higher availability of the mgr is needed, increase the count to 2.
        # In that case, one mgr will be active and one in standby. When Ceph updates which
        # mgr is active, Rook will update the mgr services to match the active mgr.
        count: 2
        allowMultiplePerNode: false
        modules:
          # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
          # are already enabled by other settings in the cluster CR.
          - name: pg_autoscaler
            enabled: true
          - name: rook
            enabled: true
          - name: telemetry
            enabled: false # Disable telemetry to avoid sending data to Ceph developers
          - name: nfs
            enabled: true # Enable NFS module

          # Add other modules if needed, e.g., 'prometheus' is implicitly enabled by monitoring.enabled=true
      crashCollector:
        disable: false
      logCollector:
        enabled: false # Keep disabled unless needed for specific debugging

      # enable the ceph dashboard for viewing cluster status
      dashboard:
        enabled: true

        # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
        urlPrefix: /
        # serve the dashboard at the given port.
        # port: 8443
        # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
        # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
        ssl: false

      # automate [data cleanup process](https://github.com/rook/rook/blob/master/Documentation/Storage-Configuration/ceph-teardown.md#delete-the-data-on-hosts) in cluster destruction.
      cleanupPolicy:
        # Since cluster cleanup is destructive to data, confirmation is required.
        # To destroy all Rook data on hosts during uninstall, confirmation must be set to "yes-really-destroy-data".
        # This value should only be set when the cluster is about to be deleted. After the confirmation is set,
        # Rook will immediately stop configuring the cluster and only wait for the delete command.
        # If the empty string is set, Rook will not destroy any data on hosts during uninstall.
        confirmation: "" # Leave empty normally

        # sanitizeDisks represents settings for sanitizing OSD disks on cluster deletion
        sanitizeDisks:
          # method indicates if the entire disk should be sanitized or simply ceph's metadata
          # in both case, re-install is possible
          # possible choices are 'complete' or 'quick' (default)
          method: quick
          # dataSource indicate where to get random bytes from to write on the disk
          # possible choices are 'zero' (default) or 'random'
          # using random sources will consume entropy from the system and will take much more time then the zero source
          dataSource: zero
          # iteration overwrite N times instead of the default (1)
          # takes an integer value
          iteration: 2
        # allowUninstallWithVolumes defines how the uninstall should be performed
        # If set to true, cephCluster deletion does not wait for the PVs to be deleted.
        allowUninstallWithVolumes: false

      placement:
        all: &placement # Placement for Mons, Mgrs, OSDs etc.
          nodeAffinity:
            requiredDuringSchedulingIgnoredDuringExecution:
              nodeSelectorTerms:
                - matchExpressions:
                    # Ensure only nodes meant for Ceph components get selected
                    # Use a generic label or specific ones per role
                    - key: ceph.rook.io/cluster
                      operator: In
                      values:
                        - "enabled"
          # Add specific placements for mon, mgr, osd if needed to separate onto nodes
          # e.g., Place OSDs only on nodes with the 'role=storage-node' label (as you had before)
          osd:
            nodeAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                nodeSelectorTerms:
                - matchExpressions:
                    - key: role # Label specific to your main storage node(s)
                      operator: In
                      values:
                        - "storage-node"

      storage:
        onlyApplyOSDPlacement: false # Allows merging general placement + OSD placement
        # --- OSD Definition ---
        # Initially target specific devices for the Pre-Check phase
        useAllNodes: false # Don't use all nodes automatically
        useAllDevices: false # Don't use all devices automatically
        nodes:
          - name: "main.storage" # Name matching your main server node
            # Use /dev/disk/by-id for stable names
            devices:
              - name: "/dev/disk/by-id/nvme-Samsung_SSD_970_EVO_Plus_1TB_S4EWNG0M218299R"
              - name: "/dev/disk/by-id/nvme-Samsung_SSD_990_EVO_2TB_S7GDNS0X203177N"
              - name: "/dev/disk/by-id/ata-ST12000NM001G-2MV103_ZS802E0G"
              - name: "/dev/disk/by-id/ata-ST8000NM0055-1RM112_ZA1DGDA3"
              - name: "/dev/disk/by-id/ata-ST8000NM0055-1RM112_ZA1FHDYF"
        config:
          osdsPerDevice: "1" # Standard config

        # storageClassDeviceSets: [] # Keep commented/removed if using raw devices

      resources: # Keep your resource requests/limits, adjust as needed
        mgr: { requests: { cpu: "125m", memory: "512Mi" }, limits: { memory: "1Gi" } }
        mon: { requests: { cpu: "50m", memory: "512Mi" }, limits: { memory: "1Gi" } }
        osd: { requests: { cpu: "300m", memory: "512Mi" }, limits: { memory: "2Gi" } } # Monitor OSD RAM, might need more w/o NVMe db cache
        mgr-sidecar: { requests: { cpu: "50m", memory: "100Mi" }, limits: { memory: "200Mi" } }
        logcollector: { requests: { cpu: "100m", memory: "100Mi" }, limits: { memory: "1Gi" } }
        prepareosd: { requests: { cpu: "250m", memory: "50Mi" } } # No limits recommended
        cleanup: { requests: { cpu: "250m", memory: "100Mi" }, limits: { memory: "1Gi" } }

    # disable defaults
    cephBlockPools: []
    cephECBlockPools: []
    cephFileSystems: []
    cephObjectStores: []

    # Snapshot Classes (keep if needed, defaults seem okay)
    cephFileSystemVolumeSnapshotClass: { enabled: false }
    cephBlockPoolsVolumeSnapshotClass: { enabled: true, name: csi-ceph-blockpool, isDefault: true, deletionPolicy: Delete }
